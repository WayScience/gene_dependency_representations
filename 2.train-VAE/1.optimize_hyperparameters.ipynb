{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hiplot\n",
    "from optuna.visualization import plot_param_importances\n",
    "import optuna\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from betavae import BetaVAE, train_vae, evaluate_vae\n",
    "from optimize_utils import get_optimize_args, objective\n",
    "\n",
    "script_directory = pathlib.Path(\"../0.data-download/scripts/\").resolve()\n",
    "sys.path.insert(0, str(script_directory))\n",
    "from data_loader import load_train_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8244309  0.5716864  0.21650638 ... 0.7044882  0.6934315  0.55542636]\n",
      " [0.60171366 0.44459793 0.49050593 ... 0.7006544  0.51975447 0.5930364 ]\n",
      " [0.55815876 0.41745156 0.55871475 ... 0.63283473 0.4719008  0.56748426]\n",
      " ...\n",
      " [0.6492675  0.49926242 0.36755386 ... 0.7904766  0.52086765 0.5231257 ]\n",
      " [0.5825433  0.5695022  0.44272694 ... 0.63639265 0.59594727 0.39246973]\n",
      " [0.68888646 0.5363674  0.46805647 ... 0.7161716  0.3728095  0.508517  ]]\n"
     ]
    }
   ],
   "source": [
    "# Load command line arguments\n",
    "args = get_optimize_args()\n",
    "\n",
    "# Load data\n",
    "data_directory = pathlib.Path(\"../0.data-download/data\").resolve()\n",
    "\n",
    "train_data, test_data, load_gene_stats = load_train_test_data(\n",
    "    data_directory, train_or_test=\"all\", load_gene_stats=True, zero_one_normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframes to tensors\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "test_tensor = torch.tensor(test_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-19 13:44:55,023] A new study created in memory with name: no-name-1b4db0a6-da45-4aef-bd85-35c5349c6e70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 182.30607361289734\n",
      "Epoch 1, Loss: 114.26435495887401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juliacurd/gene_dependency_representations/2.train-VAE/optimize_utils.py:122: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  batch_size = trial.suggest_int(\n",
      "/home/juliacurd/gene_dependency_representations/2.train-VAE/optimize_utils.py:125: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  epochs = trial.suggest_int(\n",
      "/home/juliacurd/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/optuna/distributions.py:700: UserWarning: The distribution is specified by [5, 1000] and step=100, but the range is not divisible by `step`. It will be replaced by [5, 905].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 74.8010391122879\n",
      "Epoch 3, Loss: 56.768469583197366\n",
      "Epoch 4, Loss: 49.45094130607436\n",
      "Epoch 5, Loss: 46.882413896940264\n",
      "Epoch 6, Loss: 46.01964559719077\n",
      "Epoch 7, Loss: 45.65857810880394\n",
      "Epoch 8, Loss: 45.415811824564265\n",
      "Epoch 9, Loss: 45.505688041668265\n",
      "Epoch 10, Loss: 45.37813986138571\n",
      "Epoch 11, Loss: 45.335391707736676\n",
      "Epoch 12, Loss: 45.29060127576973\n",
      "Epoch 13, Loss: 45.336144974542194\n",
      "Epoch 14, Loss: 45.300030307629186\n",
      "Epoch 15, Loss: 45.30430081904081\n",
      "Epoch 16, Loss: 45.22100305205774\n",
      "Epoch 17, Loss: 45.31579154949516\n",
      "Epoch 18, Loss: 45.33493874289773\n",
      "Epoch 19, Loss: 45.279708974777336\n",
      "Epoch 20, Loss: 45.26187966086648\n",
      "Epoch 21, Loss: 45.26349165632918\n",
      "Epoch 22, Loss: 45.26935826706945\n",
      "Epoch 23, Loss: 45.2740177088932\n",
      "Epoch 24, Loss: 45.2481993129271\n",
      "Epoch 25, Loss: 45.26036030537373\n",
      "Epoch 26, Loss: 45.27266151371986\n",
      "Epoch 27, Loss: 45.34253701999673\n",
      "Epoch 28, Loss: 45.18459738091696\n",
      "Epoch 29, Loss: 45.3213461986045\n",
      "Epoch 30, Loss: 45.26535544055686\n",
      "Epoch 31, Loss: 45.29469250578259\n",
      "Epoch 32, Loss: 45.26665772384156\n",
      "Epoch 33, Loss: 45.283089452645115\n",
      "Epoch 34, Loss: 45.24265621100948\n",
      "Epoch 35, Loss: 45.27208055500902\n",
      "Epoch 36, Loss: 45.33875898881392\n",
      "Epoch 37, Loss: 45.2437253010068\n",
      "Epoch 38, Loss: 45.33506763655079\n",
      "Epoch 39, Loss: 45.29394253817472\n",
      "Epoch 40, Loss: 45.25644573239788\n",
      "Epoch 41, Loss: 45.265938348793576\n",
      "Epoch 42, Loss: 45.20835797733991\n",
      "Epoch 43, Loss: 45.21424434577511\n",
      "Epoch 44, Loss: 45.141346849446215\n",
      "Epoch 45, Loss: 45.12531634806415\n",
      "Epoch 46, Loss: 45.355217711052674\n",
      "Epoch 47, Loss: 45.36023166665867\n",
      "Epoch 48, Loss: 45.25318784971495\n",
      "Epoch 49, Loss: 45.14058713420896\n",
      "Epoch 50, Loss: 45.26658551640241\n",
      "Epoch 51, Loss: 45.25937309546318\n",
      "Epoch 52, Loss: 45.28848963931972\n",
      "Epoch 53, Loss: 45.216909272664886\n",
      "Epoch 54, Loss: 45.28021615143197\n",
      "Epoch 55, Loss: 45.236653020985294\n",
      "Epoch 56, Loss: 45.169054410967256\n",
      "Epoch 57, Loss: 45.21693251701186\n",
      "Epoch 58, Loss: 45.22420544823499\n",
      "Epoch 59, Loss: 45.20152436427461\n",
      "Epoch 60, Loss: 45.20137620030808\n",
      "Epoch 61, Loss: 45.09895905815706\n",
      "Epoch 62, Loss: 45.29844954207137\n",
      "Epoch 63, Loss: 45.28245563179035\n",
      "Epoch 64, Loss: 45.18280126773169\n",
      "Epoch 65, Loss: 45.19967523898188\n",
      "Epoch 66, Loss: 45.333794971063035\n",
      "Epoch 67, Loss: 45.18631881226486\n",
      "Epoch 68, Loss: 45.31410768401125\n",
      "Epoch 69, Loss: 45.20168437535991\n",
      "Epoch 70, Loss: 45.313998585544\n",
      "Epoch 71, Loss: 45.21747193582521\n",
      "Epoch 72, Loss: 45.28837911619894\n",
      "Epoch 73, Loss: 45.31166425325361\n",
      "Epoch 74, Loss: 45.18168164002515\n",
      "Epoch 75, Loss: 45.15307534707559\n",
      "Epoch 76, Loss: 45.206633321776145\n",
      "Epoch 77, Loss: 45.23706909479615\n",
      "Epoch 78, Loss: 45.14956245141182\n",
      "Epoch 79, Loss: 45.25400904999904\n",
      "Epoch 80, Loss: 45.17757788805763\n",
      "Epoch 81, Loss: 45.19155148967771\n",
      "Epoch 82, Loss: 45.29389409995489\n",
      "Epoch 83, Loss: 45.20291340146076\n",
      "Epoch 84, Loss: 45.28683531665099\n",
      "Epoch 85, Loss: 45.25991258925829\n",
      "Epoch 86, Loss: 45.32526819477503\n",
      "Epoch 87, Loss: 45.28106591976831\n",
      "Epoch 88, Loss: 45.2736426501075\n",
      "Epoch 89, Loss: 45.245330135710994\n",
      "Epoch 90, Loss: 45.28311937036913\n",
      "Epoch 91, Loss: 45.23377964479039\n",
      "Epoch 92, Loss: 45.23022430944794\n",
      "Epoch 93, Loss: 45.36300824139569\n",
      "Epoch 94, Loss: 45.19785940559256\n",
      "Epoch 95, Loss: 45.372642498344405\n",
      "Epoch 96, Loss: 45.22091757284628\n",
      "Epoch 97, Loss: 45.26438157740038\n",
      "Epoch 98, Loss: 45.26692585863118\n",
      "Epoch 99, Loss: 45.19858852826992\n",
      "Epoch 100, Loss: 45.22746460560791\n",
      "Epoch 101, Loss: 45.240557546404716\n",
      "Epoch 102, Loss: 45.190202267808466\n",
      "Epoch 103, Loss: 45.217351890020346\n",
      "Epoch 104, Loss: 45.27566318371372\n",
      "Epoch 105, Loss: 45.1904990456321\n",
      "Epoch 106, Loss: 45.17882693428958\n",
      "Epoch 107, Loss: 45.220738216465755\n",
      "Epoch 108, Loss: 45.29662950970324\n",
      "Epoch 109, Loss: 45.30107105451954\n",
      "Epoch 110, Loss: 45.201936014161355\n",
      "Epoch 111, Loss: 45.15396260629415\n",
      "Epoch 112, Loss: 45.157131335659166\n",
      "Epoch 113, Loss: 45.21828496309698\n",
      "Epoch 114, Loss: 45.257801927571215\n",
      "Epoch 115, Loss: 45.14931036271979\n",
      "Epoch 116, Loss: 45.1790672508446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-07-19 13:45:04,381] Trial 0 failed with parameters: {'latent_dim': 19, 'beta': 1.9913451018512291, 'learning_rate': 0.005, 'batch_size': 16, 'epochs': 605} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/juliacurd/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_41974/4197105900.py\", line 3, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, train_tensor, test_tensor, train_data), n_trials=500)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/juliacurd/gene_dependency_representations/2.train-VAE/optimize_utils.py\", line 140, in objective\n",
      "    train_vae(model, train_loader, optimizer, epochs=epochs)\n",
      "  File \"/home/juliacurd/gene_dependency_representations/2.train-VAE/betavae.py\", line 98, in train_vae\n",
      "    optimizer.step()\n",
      "  File \"/home/juliacurd/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 379, in wrapper\n",
      "    with torch.autograd.profiler.record_function(profile_name):\n",
      "  File \"/home/juliacurd/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/torch/autograd/profiler.py\", line 605, in __enter__\n",
      "    self.record = torch.ops.profiler._record_function_enter_new(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/juliacurd/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/torch/_ops.py\", line 854, in __call__\n",
      "    return self_._op(*args, **(kwargs or {}))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-07-19 13:45:04,388] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117, Loss: 45.252715389617244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run Optuna optimization\u001b[39;00m\n\u001b[1;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run Optuna optimization\u001b[39;00m\n\u001b[1;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n",
      "File \u001b[0;32m~/gene_dependency_representations/2.train-VAE/optimize_utils.py:140\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, train_tensor, test_tensor, train_df)\u001b[0m\n\u001b[1;32m    137\u001b[0m model \u001b[38;5;241m=\u001b[39m BetaVAE(input_dim\u001b[38;5;241m=\u001b[39mtrain_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, beta\u001b[38;5;241m=\u001b[39mbeta)\n\u001b[1;32m    138\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m--> 140\u001b[0m \u001b[43mtrain_vae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Evaluate VAE\u001b[39;00m\n\u001b[1;32m    143\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate_vae(model, test_loader)\n",
      "File \u001b[0;32m~/gene_dependency_representations/2.train-VAE/betavae.py:98\u001b[0m, in \u001b[0;36mtrain_vae\u001b[0;34m(model, train_loader, optimizer, epochs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     97\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 98\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m    100\u001b[0m train_loss_history\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "File \u001b[0;32m~/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/torch/optim/optimizer.py:379\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    378\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 379\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/torch/autograd/profiler.py:605\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/gene_dependency_representations/lib/python3.12/site-packages/torch/_ops.py:854\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;66;03m# named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, train_tensor, test_tensor, train_data), n_trials=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=37, state=1, values=[101.85667588975694], datetime_start=datetime.datetime(2024, 7, 19, 11, 5, 27, 824798), datetime_complete=datetime.datetime(2024, 7, 19, 11, 5, 48, 314352), params={'latent_dim': 23, 'beta': 1.0124069329596523, 'learning_rate': 0.005, 'batch_size': 48, 'epochs': 605}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'latent_dim': IntDistribution(high=100, log=False, low=10, step=1), 'beta': FloatDistribution(high=10.0, log=False, low=1.0, step=None), 'learning_rate': CategoricalDistribution(choices=(0.005, 0.001, 0.0001, 1e-05, 1e-06)), 'batch_size': IntDistribution(high=112, log=False, low=16, step=32), 'epochs': IntDistribution(high=905, log=False, low=5, step=100)}, trial_id=37, value=None)\n",
      "Best trial: [101.85667588975694]\n",
      "Best hyperparameters: {'latent_dim': 23, 'beta': 1.0124069329596523, 'learning_rate': 0.005, 'batch_size': 48, 'epochs': 605}\n"
     ]
    }
   ],
   "source": [
    "# Save best hyperparameters\n",
    "best_trial = study.best_trial\n",
    "print(best_trial)\n",
    "print(f\"Best trial: {best_trial.values}\")\n",
    "print(f\"Best hyperparameters: {best_trial.params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gene_dependency_representations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
